{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911665e8",
   "metadata": {},
   "source": [
    "# Luentoviikko 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb673f93",
   "metadata": {},
   "source": [
    "## Luennon tärkeimmät pointit\n",
    "***\n",
    "Toisella luentoviikolla käytiin läpi eri datalähteitä, datan keräämistä käytännössä, datatieteiden prosessia ja liiketoimintamerkitystä, kirjastoja ja ohjelmia joilla käytännön datatieteiden työtä voidaan toteuttaa. Luennolla käsiteltiin myös, mitä on laadukas data sekä mitä laillisia seikkoja datan keruussa on otettava huomioon. Luentoa varten en itse ollut paikalla, vaan tarkastelin luentomuistiinpanoja netistä sekä perehdyin ennakkomateriaaliin.\n",
    "***\n",
    "#### 1:\n",
    "Datatieteiden käyttökohteet yrityksissä ovat moninaiset, ja uusia sovelluskohteita löytyy jatkuvasti. Dataa käytetäänkin laajasti päätöksenteon tukena monilla aloilla, kuten kuva 1 (1) havainnollistaa. Käyttökohteiden mukana myös datatieteilijöiltä vaadittavat taidot ovat muuttuneet, ja tarvittavien taitojen määrä on kasvanut. Virheellisesti kuitenkin monessa yrityksessä yksittäiselle datatieteilijälle asetetaan vaatimuksia, joita on käytännössä mahdoton täyttää. Ennakkomateriaalissa käytiinkin läpi menestyvän data-analytiikka tiimin vaatimuksia. Avainasemassa liiketoimintahyödyn saavuttamiseen analytiikan avulla onkin laajan osaamispohjan sisällyttäminen tiimiin siten, että tiimissä ymmärretään sekä kova datatieteiden metodologia, mutta samalla myös tutkittavan aiheen liiketoimintamerkitys ja kuinka asia tulisi kommunikoida muulle organisaatiolle.\n",
    "\n",
    "<img src=\"StatistaH2.png\">\n",
    "\n",
    "#### 2:\n",
    "Datatieteiden liiketoimintapotentiaali on suuri, mutta käyttö itsessään on monimutkainen prosessi. Prosessi alkaa datan keruulla jostain lähteestä: luennolla esiteltiin muun muassa raapijat sekä ryömijät, joiden avulla verkkosivuilta voidaan kerätä dataa. Muita lähteitä voivat olla avoimen datan lähteet, kuten esimerkiksi avoindata.fi, verkko API:t kuten Twitterin API, eri järjestelmien luomat lokitiedot, manuaalisesti luodut tietokannat tai fyysiestä työkoneesta kerätty data. Keräämisvaiheen oleellinen osa on myös datan siistiminen, sillä keräämisen jälkeen alkaa kuitenkin nopeasti syntyä ongelmia, ellei kerättystä datasta poisteta virheitä. Siistimisen jälkeen datan analysointiin voidaan luoda tarvittavat algoritmit, joita ajamalla ja iteratiivisesti kehittämällä kyetään lähestymään halutun kaltaista analytiikkaa. Kun analytiikka on olemassa, on aika ottaa askel taakse ja reflektoida, onko saatu lopputulos halutun mukainen ja vastaa tarpeeseen. Jos on, voidaan huoletta jatkaa eteenpäin. Jos kuitenkaan ei ole, pitää palata edelliseen vaiheeseen. Viimesenä vaiheena prosessissa on tärkein vaihe: tulosten kommunikointi. Kuten ennakkomateriaaleissa huomautettiin, usein moni tärkeä havainto jää käyttämättä sillä sitä ei ole saatu kommunikoitua tarpeeksi hyvin eteenpäin.\n",
    "\n",
    "#### 3:\n",
    "Datan keruuta voi toteuttaa verkkoympäristöstä web ryömijöillä ja raapijoilla. Ryömijät ovat ohjelmia, keräävät tietoa verkkosivulta ja  iteroivat annetun verkkosivun rakenteesta löytyviä linkkejä eteenpäin kunnes saavuttavat ennalta määritetyn pisteen ja lopettavat. Raapijat taas keräävät tietoa yhdeltä verkkosivulta. Luennolla annettiin esimerkkeinä useita raapijoita, joista Pythonille käytännöllinen versio on Scrapy.\n",
    "\n",
    "#### 4:\n",
    "Kuten luennolla annetussa materiaalissa tuli ilmi, datan kerääminen tällä tavoin ei ole välttämättä laillista kaikissa tilanteissa, ja paljolti riippuu sivun käyttöehdoista hyväksytäänkö tietojen raapimista. Esimerkkinä käytetty verkkokauppa.fi tiedonraavinta on esimerkki hyvästä syystä kieltää tiedon raapiminen. Potentiaaiset kilpailijat voivat saavuttaa kilpailuetua luomalla analytiikkaa saatavilla olevasta datasta. EU:ssa varsinkin henkilökohtaisen datan raapimisessa on rajoitteensa, kuten Szwed (2021) (2) tuo esille, ja raapijan avulla tiedon keruuta suunniteltaessa kannattaa laillinen taustatyö tehdä kunnolla.\n",
    "\n",
    "#### 5:\n",
    "Data tulee jalostaa aina sopivaan muotoon käytössä olevaa työkalua varten. Datasta voi jalostaa laadukkaampaa omia prosesseja varten eri menetelmillä, esimerkiksi muuttamalla tietomuotoja toisiksi- jotkin tietorakenteet toimivat paremmin kuin toiset käyttötarkoituksesta riippuen. Luennolla esimerkiksi tuotiin Pandas pythonissa, mutta analytiikkaa ja jalostamista varten on olemassa kirjastoja myös Juliassa, R- kielessä ja Javascriptissä.\n",
    "\n",
    "\n",
    "(1): https://www.statista.com/statistics/1235436/worldwide-data-driven-decision-making-organizations-by-sector/\n",
    "\n",
    "(2): https://discoverdigitallaw.com/is-web-scraping-legal-short-guide-on-scraping-under-the-eu-jurisdiction/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46151c3",
   "metadata": {},
   "source": [
    "## Kehitysvinkit\n",
    "Datan laadukkuus jäi hieman auki materiaaleista, konseptia voisi selventää hieman lisää."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a58a7",
   "metadata": {},
   "source": [
    "## Koodi\n",
    "Koodissa on yritetty käyttää scrapyä tiedoston hakemiseksi. Koodi tehty käyttäen perjantain (18.3) esimerkkikoodia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ad126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tarvittavat kirjastot jalostamiseen, keräämiseen ja tiedon säilöntään.\n",
    "import pandas as pd\n",
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b825a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Luodaan raapija w3schoolssille\n",
    "class WKolmeSchoolsSpider(scrapy.Spider):\n",
    "    name = \"WKolme\"\n",
    "    ##Sallittu domaini, jonka ulos raapija ei etene\n",
    "    allowed_domains = ['https://www.w3schools.com/']\n",
    "    \n",
    "    ##Aloitus url\n",
    "    start_urls = ['https://www.w3schools.com/python/default.asp']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        \n",
    "        courses = responses.css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72dd3162",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scrapy' has no attribute 'spider'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##Raapijan määrittely\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mQuotesSpider\u001b[39;00m(\u001b[43mscrapy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspider\u001b[49m):\n\u001b[0;32m      3\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquotes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m##URL osoitteet, josta raapija aloittaa\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scrapy' has no attribute 'spider'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8c3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
